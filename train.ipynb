{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Tpk8xFoKswgQ"},"outputs":[],"source":["import torch\n","import torchvision\n","import glob\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import random\n","import os"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"PXnXG7attgMx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666363489453,"user_tz":-540,"elapsed":2510,"user":{"displayName":"Ayato Hashimoto","userId":"17259033800136212664"}},"outputId":"1859a8c8-facf-49a2-ce82-5679bca0e59e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJWP8E4KswgS"},"outputs":[],"source":["ANSWER_SIZE = 128\n","WINDOW_SIZE = ANSWER_SIZE / 2\n","MINI_BATCH = 10\n","EPOCH = 500\n","DATASET_DIR = '/content/drive/MyDrive/Colab Notebooks/super_resolution_ai/dataset/images/*'\n","OUTPUT_DIR = '/content/drive/MyDrive/Colab Notebooks/super_resolution_ai/outputs/'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwmrQreFswgS","executionInfo":{"status":"ok","timestamp":1666363489454,"user_tz":-540,"elapsed":16,"user":{"displayName":"Ayato Hashimoto","userId":"17259033800136212664"}},"outputId":"878c7368-a59a-4a7b-ad89-1cf2feb88bf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["30 images loaded.(train:valid=20:10)\n"]}],"source":["files = glob.glob(DATASET_DIR)\n","train_files = files[0:20]\n","valid_files = files[20:30]\n","print('{} images loaded.(train:valid={}:{})'.format(len(files),len(train_files),len(valid_files)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x-x6IISTswgT"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, files):\n","        self.files = files\n","        self.toTensor = torchvision.transforms.ToTensor()\n","        self.randomCrop = torchvision.transforms.RandomCrop(ANSWER_SIZE, padding = ANSWER_SIZE-1)\n","        self.augmentation = torchvision.transforms.Compose([\n","            torchvision.transforms.ColorJitter(),\n","            torchvision.transforms.RandomGrayscale(p=0.1),\n","            torchvision.transforms.RandomHorizontalFlip(),\n","            torchvision.transforms.RandomVerticalFlip(),\n","            torchvision.transforms.RandomInvert(p=0.1)\n","        ])\n","        self.downsize = torchvision.transforms.Resize(int(ANSWER_SIZE/2))\n","        self.upsize = torchvision.transforms.Resize(int(ANSWER_SIZE))\n","\n","    def __len__(self):\n","        return len(self.files)\n","\n","    def __getitem__(self, idx):\n","        # read image\n","        image = Image.open(self.files[idx])\n","        image = self.toTensor(image)\n","        # random crop\n","        image = self.randomCrop(image)\n","        # augmentation\n","        large = self.augmentation(image)\n","        # downsize\n","        small = self.downsize(large)\n","        small = self.upsize(small)\n","        return small, large"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"7Tq19LRaswgU","executionInfo":{"status":"ok","timestamp":1666382169462,"user_tz":-540,"elapsed":320,"user":{"displayName":"Ayato Hashimoto","userId":"17259033800136212664"}}},"outputs":[],"source":["class UpSampleNet(torch.nn.Module):\n","    def __init__(self):\n","        super(UpSampleNet, self).__init__()\n","        self.input = self.output = 0\n","        self.layer1 = self.layer2 = self.layer3 = self.layer4 = self.layer5 = 0\n","        \n","        self.step1 = torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n","            torch.nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n","        )\n","\n","        self.step2 = torch.nn.Sequential(\n","            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n","            torch.nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n","            torch.nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n","        )\n","\n","        self.step3 = torch.nn.Sequential(\n","            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n","            torch.nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(128), torch.nn.ReLU(),\n","            torch.nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(128), torch.nn.ReLU(),\n","        )\n","\n","        self.deconv1 = torch.nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n","\n","        self.step4 = torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n","            torch.nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n","        )\n","\n","        self.deconv2 = torch.nn.ConvTranspose2d(in_channels=64,out_channels=32,kernel_size=2,stride=2)\n","\n","        self.step5 = torch.nn.Sequential(\n","            torch.nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n","            torch.nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1),\n","            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n","            torch.nn.Conv2d(in_channels=32,out_channels=3,kernel_size=3,padding=1),\n","            torch.nn.Sigmoid()\n","        )\n","\n","    def forward(self, small):\n","        self.input = small\n","        # Encode\n","        self.layer1 = self.step1(self.input)\n","        self.layer2 = self.step2(self.layer1)\n","        self.layer3 = self.step3(self.layer2)\n","        # Decode\n","        self.layer3 = self.deconv1(self.layer3)\n","        self.layer4 = self.step4(torch.cat((self.layer3,self.layer2),dim=1))\n","        self.layer4 = self.deconv2(self.layer4)\n","        self.layer5 = self.step5(torch.cat((self.layer4,self.layer1),dim=1))\n","        self.output = self.layer5\n","        return self.output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IDxMb9neswgU"},"outputs":[],"source":["train_dataset, valid_dataset = Dataset(train_files), Dataset(valid_files)\n","train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=MINI_BATCH,num_workers=os.cpu_count(),pin_memory=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=MINI_BATCH,num_workers=os.cpu_count(),pin_memory=True)\n","train_history, valid_history = {'loss':[]}, {'loss':[]}\n","\n","net = UpSampleNet()\n","optim = torch.optim.Adam(params=net.parameters(),lr=0.0001)\n","loss_function = torch.nn.MSELoss()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6dDL9WtswgU"},"outputs":[],"source":["def train_valid(mode, model, loader, loss_function, optim, history):\n","    train = True if mode == 'train' else False\n","    model.train(train)\n","    loss_sum = 0\n","\n","    for small, large in loader:\n","        if train: optim.zero_grad()\n","        outputs = model(small)\n","        loss = loss_function(outputs, large)\n","\n","        loss_sum += loss.item()\n","\n","        if train:\n","            loss.backward()\n","            optim.step()\n","        \n","    history['loss'].append(loss_sum / len(loader))\n","    \n","    log = '[Train]' if train else '[Valid]'\n","    log += ' loss:' + str(history['loss'][-1])\n","    print(log)\n","\n","def plot_graph(train_loss, valid_loss):\n","    plt.figure()\n","    plt.ioff()\n","    plt.plot(range(1, len(train_loss)+1), train_loss, label = 'train')\n","    plt.plot(range(1, len(valid_loss)+1), valid_loss, label = 'valid')\n","    plt.title('Loss')\n","    plt.xlabel('epoch')\n","    plt.legend()\n","    plt.savefig(OUTPUT_DIR + 'valid/' + '{}epoch_loss.png'.format(len(train_loss)), facecolor='white')\n","    plt.clf()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NVlib6znswgV","outputId":"3e51bf47-fe07-4c3a-b39b-307e6567dcb2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1666368320571,"user_tz":-540,"elapsed":4831129,"user":{"displayName":"Ayato Hashimoto","userId":"17259033800136212664"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--- 1 Epoch ---\n","[Train] loss:0.140010304749012\n","[Valid] loss:0.10648511350154877\n","--- 2 Epoch ---\n","[Train] loss:0.11071434617042542\n","[Valid] loss:0.14306935667991638\n","--- 3 Epoch ---\n","[Train] loss:0.12647808343172073\n","[Valid] loss:0.13729329407215118\n","--- 4 Epoch ---\n","[Train] loss:0.10051754489541054\n","[Valid] loss:0.1196698248386383\n","--- 5 Epoch ---\n","[Train] loss:0.09029032289981842\n","[Valid] loss:0.07534029334783554\n","--- 6 Epoch ---\n","[Train] loss:0.09513825178146362\n","[Valid] loss:0.15446002781391144\n","--- 7 Epoch ---\n","[Train] loss:0.06996199674904346\n","[Valid] loss:0.14027120172977448\n","--- 8 Epoch ---\n","[Train] loss:0.07038303650915623\n","[Valid] loss:0.10818600654602051\n","--- 9 Epoch ---\n","[Train] loss:0.0647780504077673\n","[Valid] loss:0.10142547637224197\n","--- 10 Epoch ---\n","[Train] loss:0.05939783528447151\n","[Valid] loss:0.12494337558746338\n","--- 11 Epoch ---\n","[Train] loss:0.05139869451522827\n","[Valid] loss:0.0863843634724617\n","--- 12 Epoch ---\n","[Train] loss:0.06841539032757282\n","[Valid] loss:0.08641985803842545\n","--- 13 Epoch ---\n","[Train] loss:0.06042514927685261\n","[Valid] loss:0.08802981674671173\n","--- 14 Epoch ---\n","[Train] loss:0.051364198327064514\n","[Valid] loss:0.08415654301643372\n","--- 15 Epoch ---\n","[Train] loss:0.03790813870728016\n","[Valid] loss:0.07807496190071106\n","--- 16 Epoch ---\n","[Train] loss:0.04791455529630184\n","[Valid] loss:0.07372927665710449\n","--- 17 Epoch ---\n","[Train] loss:0.04607590101659298\n","[Valid] loss:0.06664325296878815\n","--- 18 Epoch ---\n","[Train] loss:0.04512770660221577\n","[Valid] loss:0.06843923032283783\n","--- 19 Epoch ---\n","[Train] loss:0.06899985857307911\n","[Valid] loss:0.061639104038476944\n","--- 20 Epoch ---\n","[Train] loss:0.03147363290190697\n","[Valid] loss:0.05443689972162247\n","--- 21 Epoch ---\n","[Train] loss:0.041519155725836754\n","[Valid] loss:0.0453072153031826\n","--- 22 Epoch ---\n","[Train] loss:0.033635761588811874\n","[Valid] loss:0.037979163229465485\n","--- 23 Epoch ---\n","[Train] loss:0.05229424126446247\n","[Valid] loss:0.03122759610414505\n","--- 24 Epoch ---\n","[Train] loss:0.03334320057183504\n","[Valid] loss:0.033387117087841034\n","--- 25 Epoch ---\n","[Train] loss:0.044002363458275795\n","[Valid] loss:0.033589836210012436\n","--- 26 Epoch ---\n","[Train] loss:0.03372057620435953\n","[Valid] loss:0.034786246716976166\n","--- 27 Epoch ---\n","[Train] loss:0.02667477075010538\n","[Valid] loss:0.03538109362125397\n","--- 28 Epoch ---\n","[Train] loss:0.027580912224948406\n","[Valid] loss:0.027185816317796707\n","--- 29 Epoch ---\n","[Train] loss:0.026326929219067097\n","[Valid] loss:0.0228303000330925\n","--- 30 Epoch ---\n","[Train] loss:0.019932745955884457\n","[Valid] loss:0.0280044786632061\n","--- 31 Epoch ---\n","[Train] loss:0.022718995809555054\n","[Valid] loss:0.021001650020480156\n","--- 32 Epoch ---\n","[Train] loss:0.022113188169896603\n","[Valid] loss:0.027576375752687454\n","--- 33 Epoch ---\n","[Train] loss:0.01632645819336176\n","[Valid] loss:0.02316921204328537\n","--- 34 Epoch ---\n","[Train] loss:0.03264417685568333\n","[Valid] loss:0.018004687502980232\n","--- 35 Epoch ---\n","[Train] loss:0.028469030745327473\n","[Valid] loss:0.020478451624512672\n","--- 36 Epoch ---\n","[Train] loss:0.04612935706973076\n","[Valid] loss:0.02424902841448784\n","--- 37 Epoch ---\n","[Train] loss:0.032671134918928146\n","[Valid] loss:0.020641591399908066\n","--- 38 Epoch ---\n","[Train] loss:0.020190320909023285\n","[Valid] loss:0.020971445366740227\n","--- 39 Epoch ---\n","[Train] loss:0.021353688091039658\n","[Valid] loss:0.017467524856328964\n","--- 40 Epoch ---\n","[Train] loss:0.02372868452221155\n","[Valid] loss:0.013083218596875668\n","--- 41 Epoch ---\n","[Train] loss:0.015301204286515713\n","[Valid] loss:0.018140502274036407\n","--- 42 Epoch ---\n","[Train] loss:0.016709926072508097\n","[Valid] loss:0.01256085280328989\n","--- 43 Epoch ---\n","[Train] loss:0.020746289752423763\n","[Valid] loss:0.01499185524880886\n","--- 44 Epoch ---\n","[Train] loss:0.01146572595462203\n","[Valid] loss:0.011972326785326004\n","--- 45 Epoch ---\n","[Train] loss:0.021703985054045916\n","[Valid] loss:0.016954902559518814\n","--- 46 Epoch ---\n","[Train] loss:0.028696645982563496\n","[Valid] loss:0.014904932118952274\n","--- 47 Epoch ---\n","[Train] loss:0.012324566021561623\n","[Valid] loss:0.012570170685648918\n","--- 48 Epoch ---\n","[Train] loss:0.022490947507321835\n","[Valid] loss:0.014320969581604004\n","--- 49 Epoch ---\n","[Train] loss:0.022433285601437092\n","[Valid] loss:0.01537076011300087\n","--- 50 Epoch ---\n","[Train] loss:0.024248508736491203\n","[Valid] loss:0.01582249253988266\n","--- 51 Epoch ---\n","[Train] loss:0.014982813037931919\n","[Valid] loss:0.015245920978486538\n","--- 52 Epoch ---\n","[Train] loss:0.016170557588338852\n","[Valid] loss:0.014523694291710854\n","--- 53 Epoch ---\n","[Train] loss:0.022146335802972317\n","[Valid] loss:0.011088998056948185\n","--- 54 Epoch ---\n","[Train] loss:0.01645063841715455\n","[Valid] loss:0.015435073524713516\n","--- 55 Epoch ---\n","[Train] loss:0.012582863681018353\n","[Valid] loss:0.011522234417498112\n","--- 56 Epoch ---\n","[Train] loss:0.016882361378520727\n","[Valid] loss:0.00986390095204115\n","--- 57 Epoch ---\n","[Train] loss:0.013462919276207685\n","[Valid] loss:0.011172701604664326\n","--- 58 Epoch ---\n","[Train] loss:0.014640734996646643\n","[Valid] loss:0.01197029184550047\n","--- 59 Epoch ---\n","[Train] loss:0.01652136817574501\n","[Valid] loss:0.011976792477071285\n","--- 60 Epoch ---\n","[Train] loss:0.009159152396023273\n","[Valid] loss:0.008650965057313442\n","--- 61 Epoch ---\n","[Train] loss:0.011122191324830055\n","[Valid] loss:0.014088737778365612\n","--- 62 Epoch ---\n","[Train] loss:0.015598179306834936\n","[Valid] loss:0.008205564692616463\n","--- 63 Epoch ---\n","[Train] loss:0.010290735866874456\n","[Valid] loss:0.008700414560735226\n","--- 64 Epoch ---\n","[Train] loss:0.006890378776006401\n","[Valid] loss:0.009992592968046665\n","--- 65 Epoch ---\n","[Train] loss:0.010977896163240075\n","[Valid] loss:0.011792636476457119\n","--- 66 Epoch ---\n","[Train] loss:0.01072318758815527\n","[Valid] loss:0.00715036503970623\n","--- 67 Epoch ---\n","[Train] loss:0.012355111539363861\n","[Valid] loss:0.009271394461393356\n","--- 68 Epoch ---\n","[Train] loss:0.02757628308609128\n","[Valid] loss:0.011271455325186253\n","--- 69 Epoch ---\n","[Train] loss:0.008445778395980597\n","[Valid] loss:0.014126828871667385\n","--- 70 Epoch ---\n","[Train] loss:0.01138650393113494\n","[Valid] loss:0.009526927024126053\n","--- 71 Epoch ---\n","[Train] loss:0.008199376985430717\n","[Valid] loss:0.008167830295860767\n","--- 72 Epoch ---\n","[Train] loss:0.008922797162085772\n","[Valid] loss:0.008737919852137566\n","--- 73 Epoch ---\n","[Train] loss:0.014132063603028655\n","[Valid] loss:0.009398051537573338\n","--- 74 Epoch ---\n","[Train] loss:0.01474325847811997\n","[Valid] loss:0.008288616314530373\n","--- 75 Epoch ---\n","[Train] loss:0.01366362115368247\n","[Valid] loss:0.006898331921547651\n","--- 76 Epoch ---\n","[Train] loss:0.012957867933437228\n","[Valid] loss:0.007073867600411177\n","--- 77 Epoch ---\n","[Train] loss:0.008008494507521391\n","[Valid] loss:0.009920593351125717\n","--- 78 Epoch ---\n","[Train] loss:0.03159333858639002\n","[Valid] loss:0.0076649850234389305\n","--- 79 Epoch ---\n","[Train] loss:0.010598422028124332\n","[Valid] loss:0.007840736769139767\n","--- 80 Epoch ---\n","[Train] loss:0.010578178334981203\n","[Valid] loss:0.011187022551894188\n","--- 81 Epoch ---\n","[Train] loss:0.0183125794865191\n","[Valid] loss:0.009782263077795506\n","--- 82 Epoch ---\n","[Train] loss:0.02305856579914689\n","[Valid] loss:0.009977464564144611\n","--- 83 Epoch ---\n","[Train] loss:0.015308437868952751\n","[Valid] loss:0.006015458144247532\n","--- 84 Epoch ---\n","[Train] loss:0.021442661993205547\n","[Valid] loss:0.007088528946042061\n","--- 85 Epoch ---\n","[Train] loss:0.008252389496192336\n","[Valid] loss:0.008482967503368855\n","--- 86 Epoch ---\n","[Train] loss:0.007932663895189762\n","[Valid] loss:0.00839716661721468\n","--- 87 Epoch ---\n","[Train] loss:0.011871287133544683\n","[Valid] loss:0.008541163988411427\n","--- 88 Epoch ---\n","[Train] loss:0.007254691096022725\n","[Valid] loss:0.0093615110963583\n","--- 89 Epoch ---\n","[Train] loss:0.006128750741481781\n","[Valid] loss:0.007579402066767216\n","--- 90 Epoch ---\n","[Train] loss:0.007913192501291633\n","[Valid] loss:0.00814224872738123\n","--- 91 Epoch ---\n","[Train] loss:0.014333098428323865\n","[Valid] loss:0.007933644577860832\n","--- 92 Epoch ---\n","[Train] loss:0.007414191961288452\n","[Valid] loss:0.006558257155120373\n","--- 93 Epoch ---\n","[Train] loss:0.011471440084278584\n","[Valid] loss:0.006418752949684858\n","--- 94 Epoch ---\n","[Train] loss:0.004474380402825773\n","[Valid] loss:0.009165976196527481\n","--- 95 Epoch ---\n","[Train] loss:0.004021223750896752\n","[Valid] loss:0.007980119436979294\n","--- 96 Epoch ---\n","[Train] loss:0.009152894373983145\n","[Valid] loss:0.007466823793947697\n","--- 97 Epoch ---\n","[Train] loss:0.0042588707292452455\n","[Valid] loss:0.005461227614432573\n","--- 98 Epoch ---\n","[Train] loss:0.016873126849532127\n","[Valid] loss:0.007987158372998238\n","--- 99 Epoch ---\n","[Train] loss:0.008031296078115702\n","[Valid] loss:0.005639510229229927\n","--- 100 Epoch ---\n","[Train] loss:0.011886257212609053\n","[Valid] loss:0.009151545353233814\n","--- 101 Epoch ---\n","[Train] loss:0.005305075086653233\n","[Valid] loss:0.007642137818038464\n","--- 102 Epoch ---\n","[Train] loss:0.0049969409592449665\n","[Valid] loss:0.0067094615660607815\n","--- 103 Epoch ---\n","[Train] loss:0.009696494322270155\n","[Valid] loss:0.009424800984561443\n","--- 104 Epoch ---\n","[Train] loss:0.005811505485326052\n","[Valid] loss:0.005903187673538923\n","--- 105 Epoch ---\n","[Train] loss:0.0082747433334589\n","[Valid] loss:0.0047827670350670815\n","--- 106 Epoch ---\n","[Train] loss:0.006873184349387884\n","[Valid] loss:0.004639171529561281\n","--- 107 Epoch ---\n","[Train] loss:0.010547773214057088\n","[Valid] loss:0.004303785506635904\n","--- 108 Epoch ---\n","[Train] loss:0.023057962767779827\n","[Valid] loss:0.0057398914359509945\n","--- 109 Epoch ---\n","[Train] loss:0.008033840917050838\n","[Valid] loss:0.006923029664903879\n","--- 110 Epoch ---\n","[Train] loss:0.009424903197214007\n","[Valid] loss:0.00685364194214344\n","--- 111 Epoch ---\n","[Train] loss:0.006019978784024715\n","[Valid] loss:0.0069975401274859905\n","--- 112 Epoch ---\n","[Train] loss:0.004776491899974644\n","[Valid] loss:0.006074203178286552\n","--- 113 Epoch ---\n","[Train] loss:0.012138607213273644\n","[Valid] loss:0.006419797893613577\n","--- 114 Epoch ---\n","[Train] loss:0.012992569478228688\n","[Valid] loss:0.00564613239839673\n","--- 115 Epoch ---\n","[Train] loss:0.010236621368676424\n","[Valid] loss:0.005715469364076853\n","--- 116 Epoch ---\n","[Train] loss:0.004778565838932991\n","[Valid] loss:0.0073831514455378056\n","--- 117 Epoch ---\n","[Train] loss:0.006651936797425151\n","[Valid] loss:0.006149369291961193\n","--- 118 Epoch ---\n","[Train] loss:0.009495107689872384\n","[Valid] loss:0.005287247709929943\n","--- 119 Epoch ---\n","[Train] loss:0.006579023553058505\n","[Valid] loss:0.006257861386984587\n","--- 120 Epoch ---\n","[Train] loss:0.0064988345839083195\n","[Valid] loss:0.004784685559570789\n","--- 121 Epoch ---\n","[Train] loss:0.005265678279101849\n","[Valid] loss:0.006971435155719519\n","--- 122 Epoch ---\n","[Train] loss:0.01147351786494255\n","[Valid] loss:0.005758301354944706\n","--- 123 Epoch ---\n","[Train] loss:0.01318146730773151\n","[Valid] loss:0.006193031556904316\n","--- 124 Epoch ---\n","[Train] loss:0.006336943944916129\n","[Valid] loss:0.005101565737277269\n","--- 125 Epoch ---\n","[Train] loss:0.04023807658813894\n","[Valid] loss:0.01004639733582735\n","--- 126 Epoch ---\n","[Train] loss:0.007366998586803675\n","[Valid] loss:0.005518838297575712\n","--- 127 Epoch ---\n","[Train] loss:0.012378028826788068\n","[Valid] loss:0.00781610794365406\n","--- 128 Epoch ---\n","[Train] loss:0.006272545550018549\n","[Valid] loss:0.005327568855136633\n","--- 129 Epoch ---\n","[Train] loss:0.016893090680241585\n","[Valid] loss:0.004846209194511175\n","--- 130 Epoch ---\n","[Train] loss:0.006524468306452036\n","[Valid] loss:0.009312869980931282\n","--- 131 Epoch ---\n","[Train] loss:0.007580742239952087\n","[Valid] loss:0.008228767663240433\n","--- 132 Epoch ---\n","[Train] loss:0.003220958635210991\n","[Valid] loss:0.009026703424751759\n","--- 133 Epoch ---\n","[Train] loss:0.008428585017099977\n","[Valid] loss:0.006811211351305246\n","--- 134 Epoch ---\n","[Train] loss:0.004041001433506608\n","[Valid] loss:0.007283004466444254\n","--- 135 Epoch ---\n","[Train] loss:0.0068197171203792095\n","[Valid] loss:0.004773385357111692\n","--- 136 Epoch ---\n","[Train] loss:0.006253833649680018\n","[Valid] loss:0.0038848756812512875\n","--- 137 Epoch ---\n","[Train] loss:0.008080309140495956\n","[Valid] loss:0.00602631876245141\n","--- 138 Epoch ---\n","[Train] loss:0.005649613682180643\n","[Valid] loss:0.00370543054305017\n","--- 139 Epoch ---\n","[Train] loss:0.007287489483132958\n","[Valid] loss:0.0061116935685276985\n","--- 140 Epoch ---\n","[Train] loss:0.004511330043897033\n","[Valid] loss:0.004343867767602205\n","--- 141 Epoch ---\n","[Train] loss:0.004151784116402268\n","[Valid] loss:0.004762195982038975\n","--- 142 Epoch ---\n","[Train] loss:0.011332972208037972\n","[Valid] loss:0.004283486865460873\n","--- 143 Epoch ---\n","[Train] loss:0.004862429341301322\n","[Valid] loss:0.00456213066354394\n","--- 144 Epoch ---\n","[Train] loss:0.010867184610106051\n","[Valid] loss:0.0053497240878641605\n","--- 145 Epoch ---\n","[Train] loss:0.0038485131226480007\n","[Valid] loss:0.00587219325825572\n","--- 146 Epoch ---\n","[Train] loss:0.0134959357092157\n","[Valid] loss:0.007253463380038738\n","--- 147 Epoch ---\n","[Train] loss:0.013622137485072017\n","[Valid] loss:0.005010278429836035\n","--- 148 Epoch ---\n","[Train] loss:0.006193661596626043\n","[Valid] loss:0.004729316104203463\n","--- 149 Epoch ---\n","[Train] loss:0.00619526591617614\n","[Valid] loss:0.0046514105051755905\n","--- 150 Epoch ---\n","[Train] loss:0.005891428794711828\n","[Valid] loss:0.009469264186918736\n","--- 151 Epoch ---\n","[Train] loss:0.005985410185530782\n","[Valid] loss:0.005442775320261717\n","--- 152 Epoch ---\n","[Train] loss:0.0037835523253306746\n","[Valid] loss:0.0055906204506754875\n","--- 153 Epoch ---\n","[Train] loss:0.021067279390990734\n","[Valid] loss:0.003296496579423547\n","--- 154 Epoch ---\n","[Train] loss:0.013085292186588049\n","[Valid] loss:0.006172663997858763\n","--- 155 Epoch ---\n","[Train] loss:0.006089721107855439\n","[Valid] loss:0.006105450913310051\n","--- 156 Epoch ---\n","[Train] loss:0.005972690414637327\n","[Valid] loss:0.006593700032681227\n","--- 157 Epoch ---\n","[Train] loss:0.012416444718837738\n","[Valid] loss:0.00455428846180439\n","--- 158 Epoch ---\n","[Train] loss:0.006106068380177021\n","[Valid] loss:0.004267941229045391\n","--- 159 Epoch ---\n","[Train] loss:0.005559694021940231\n","[Valid] loss:0.004774327855557203\n","--- 160 Epoch ---\n","[Train] loss:0.005704293260350823\n","[Valid] loss:0.0055862474255263805\n","--- 161 Epoch ---\n","[Train] loss:0.008275863714516163\n","[Valid] loss:0.0032504384871572256\n","--- 162 Epoch ---\n","[Train] loss:0.016407048678956926\n","[Valid] loss:0.0030664182268083096\n","--- 163 Epoch ---\n","[Train] loss:0.0069038987858220935\n","[Valid] loss:0.005206934176385403\n","--- 164 Epoch ---\n","[Train] loss:0.004556762985885143\n","[Valid] loss:0.003741226391866803\n","--- 165 Epoch ---\n","[Train] loss:0.00456808740273118\n","[Valid] loss:0.0037547508254647255\n","--- 166 Epoch ---\n","[Train] loss:0.004213040578179061\n","[Valid] loss:0.003625151701271534\n","--- 167 Epoch ---\n","[Train] loss:0.005135050276294351\n","[Valid] loss:0.004803482908755541\n","--- 168 Epoch ---\n","[Train] loss:0.0060973139479756355\n","[Valid] loss:0.0038832661230117083\n","--- 169 Epoch ---\n","[Train] loss:0.006756034912541509\n","[Valid] loss:0.004915871657431126\n","--- 170 Epoch ---\n","[Train] loss:0.0052506569772958755\n","[Valid] loss:0.0022977995686233044\n","--- 171 Epoch ---\n","[Train] loss:0.004874983802437782\n","[Valid] loss:0.004670559428632259\n","--- 172 Epoch ---\n","[Train] loss:0.007974413922056556\n","[Valid] loss:0.0038041674997657537\n","--- 173 Epoch ---\n","[Train] loss:0.012149988440796733\n","[Valid] loss:0.006393758580088615\n","--- 174 Epoch ---\n","[Train] loss:0.005095028085634112\n","[Valid] loss:0.002396402647718787\n","--- 175 Epoch ---\n","[Train] loss:0.009767952840775251\n","[Valid] loss:0.003111251862719655\n","--- 176 Epoch ---\n","[Train] loss:0.008325985865667462\n","[Valid] loss:0.00500840088352561\n","--- 177 Epoch ---\n","[Train] loss:0.0072711852844804525\n","[Valid] loss:0.00914660282433033\n","--- 178 Epoch ---\n","[Train] loss:0.007987367222085595\n","[Valid] loss:0.004580641631036997\n","--- 179 Epoch ---\n","[Train] loss:0.00527830864302814\n","[Valid] loss:0.006009431090205908\n","--- 180 Epoch ---\n","[Train] loss:0.0045845836866647005\n","[Valid] loss:0.005799109116196632\n","--- 181 Epoch ---\n","[Train] loss:0.0025957468897104263\n","[Valid] loss:0.005694770719856024\n","--- 182 Epoch ---\n","[Train] loss:0.007485079928301275\n","[Valid] loss:0.006708402652293444\n","--- 183 Epoch ---\n","[Train] loss:0.011635950300842524\n","[Valid] loss:0.002804081654176116\n","--- 184 Epoch ---\n","[Train] loss:0.01273448287975043\n","[Valid] loss:0.0063378955237567425\n","--- 185 Epoch ---\n","[Train] loss:0.016766843385994434\n","[Valid] loss:0.005008752923458815\n","--- 186 Epoch ---\n","[Train] loss:0.003361909417435527\n","[Valid] loss:0.0042626503854990005\n","--- 187 Epoch ---\n","[Train] loss:0.004206821206025779\n","[Valid] loss:0.0036220571491867304\n","--- 188 Epoch ---\n","[Train] loss:0.0034452708205208182\n","[Valid] loss:0.004765614867210388\n","--- 189 Epoch ---\n","[Train] loss:0.0037417648127302527\n","[Valid] loss:0.0026384901721030474\n","--- 190 Epoch ---\n","[Train] loss:0.0040468048537150025\n","[Valid] loss:0.0027863800060003996\n","--- 191 Epoch ---\n","[Train] loss:0.007166572380810976\n","[Valid] loss:0.003920684102922678\n","--- 192 Epoch ---\n","[Train] loss:0.0053301595617085695\n","[Valid] loss:0.0043922602199018\n","--- 193 Epoch ---\n","[Train] loss:0.007295239716768265\n","[Valid] loss:0.004231445956975222\n","--- 194 Epoch ---\n","[Train] loss:0.0036858939565718174\n","[Valid] loss:0.005698754917830229\n","--- 195 Epoch ---\n","[Train] loss:0.004501479445025325\n","[Valid] loss:0.0035742854233831167\n","--- 196 Epoch ---\n","[Train] loss:0.003912778222002089\n","[Valid] loss:0.002775941276922822\n","--- 197 Epoch ---\n","[Train] loss:0.0037351748906075954\n","[Valid] loss:0.004681873135268688\n","--- 198 Epoch ---\n","[Train] loss:0.003042374853976071\n","[Valid] loss:0.0035300184972584248\n","--- 199 Epoch ---\n","[Train] loss:0.003241417813114822\n","[Valid] loss:0.004366639070212841\n","--- 200 Epoch ---\n","[Train] loss:0.005461045540869236\n","[Valid] loss:0.0045781382359564304\n","--- 201 Epoch ---\n","[Train] loss:0.003266348270699382\n","[Valid] loss:0.00484255189076066\n","--- 202 Epoch ---\n","[Train] loss:0.003148574149236083\n","[Valid] loss:0.003419447923079133\n","--- 203 Epoch ---\n","[Train] loss:0.003396901418454945\n","[Valid] loss:0.005074698943644762\n","--- 204 Epoch ---\n","[Train] loss:0.010845302836969495\n","[Valid] loss:0.003705451497808099\n","--- 205 Epoch ---\n","[Train] loss:0.002857368905097246\n","[Valid] loss:0.003966690972447395\n","--- 206 Epoch ---\n","[Train] loss:0.010739896562881768\n","[Valid] loss:0.004521960858255625\n","--- 207 Epoch ---\n","[Train] loss:0.004287373274564743\n","[Valid] loss:0.006130664609372616\n","--- 208 Epoch ---\n","[Train] loss:0.007510114926844835\n","[Valid] loss:0.00906551443040371\n","--- 209 Epoch ---\n","[Train] loss:0.006726119900122285\n","[Valid] loss:0.004749649204313755\n","--- 210 Epoch ---\n","[Train] loss:0.006927387090399861\n","[Valid] loss:0.0043463376350700855\n","--- 211 Epoch ---\n","[Train] loss:0.020372295286506414\n","[Valid] loss:0.008693679235875607\n","--- 212 Epoch ---\n","[Train] loss:0.0057753827422857285\n","[Valid] loss:0.004657970741391182\n","--- 213 Epoch ---\n","[Train] loss:0.005273125832900405\n","[Valid] loss:0.006356409285217524\n","--- 214 Epoch ---\n","[Train] loss:0.00714564323425293\n","[Valid] loss:0.0054879277013242245\n","--- 215 Epoch ---\n","[Train] loss:0.00562837696634233\n","[Valid] loss:0.006466495804488659\n","--- 216 Epoch ---\n","[Train] loss:0.003801645594649017\n","[Valid] loss:0.005370578728616238\n","--- 217 Epoch ---\n","[Train] loss:0.0034965123049914837\n","[Valid] loss:0.005328427534550428\n","--- 218 Epoch ---\n","[Train] loss:0.003722267458215356\n","[Valid] loss:0.005120777990669012\n","--- 219 Epoch ---\n","[Train] loss:0.0029405142413452268\n","[Valid] loss:0.005695436615496874\n","--- 220 Epoch ---\n","[Train] loss:0.0042364648543298244\n","[Valid] loss:0.0036522718146443367\n","--- 221 Epoch ---\n","[Train] loss:0.00517085159663111\n","[Valid] loss:0.0035908592399209738\n","--- 222 Epoch ---\n","[Train] loss:0.003422838053666055\n","[Valid] loss:0.004495380446314812\n","--- 223 Epoch ---\n","[Train] loss:0.0028855173150077462\n","[Valid] loss:0.004521743860095739\n","--- 224 Epoch ---\n","[Train] loss:0.0031385032925754786\n","[Valid] loss:0.005128537770360708\n","--- 225 Epoch ---\n","[Train] loss:0.015806682873517275\n","[Valid] loss:0.0039696283638477325\n","--- 226 Epoch ---\n","[Train] loss:0.005563638638705015\n","[Valid] loss:0.0037622794043272734\n","--- 227 Epoch ---\n","[Train] loss:0.003922677598893642\n","[Valid] loss:0.0029839996714144945\n","--- 228 Epoch ---\n","[Train] loss:0.0073404922150075436\n","[Valid] loss:0.004505851771682501\n","--- 229 Epoch ---\n","[Train] loss:0.004369944334030151\n","[Valid] loss:0.004236855078488588\n","--- 230 Epoch ---\n","[Train] loss:0.008840695838443935\n","[Valid] loss:0.0027636003214865923\n","--- 231 Epoch ---\n","[Train] loss:0.008464371552690864\n","[Valid] loss:0.004185966681689024\n","--- 232 Epoch ---\n","[Train] loss:0.005246073822490871\n","[Valid] loss:0.006808089092373848\n","--- 233 Epoch ---\n","[Train] loss:0.0035883589880540967\n","[Valid] loss:0.00421815225854516\n","--- 234 Epoch ---\n","[Train] loss:0.013298648409545422\n","[Valid] loss:0.006433500442653894\n","--- 235 Epoch ---\n","[Train] loss:0.007876552175730467\n","[Valid] loss:0.005417472682893276\n","--- 236 Epoch ---\n","[Train] loss:0.00657164363656193\n","[Valid] loss:0.002915768651291728\n","--- 237 Epoch ---\n","[Train] loss:0.005569703294895589\n","[Valid] loss:0.003677648724988103\n","--- 238 Epoch ---\n","[Train] loss:0.00535414763726294\n","[Valid] loss:0.005641396623104811\n","--- 239 Epoch ---\n","[Train] loss:0.003260397119447589\n","[Valid] loss:0.004708726890385151\n","--- 240 Epoch ---\n","[Train] loss:0.004936914308927953\n","[Valid] loss:0.0055725728161633015\n","--- 241 Epoch ---\n","[Train] loss:0.002952476148493588\n","[Valid] loss:0.0059573110193014145\n","--- 242 Epoch ---\n","[Train] loss:0.002450555213727057\n","[Valid] loss:0.004858344793319702\n","--- 243 Epoch ---\n","[Train] loss:0.004337776685133576\n","[Valid] loss:0.0035354795400053263\n","--- 244 Epoch ---\n","[Train] loss:0.005969458492472768\n","[Valid] loss:0.0028848128858953714\n","--- 245 Epoch ---\n","[Train] loss:0.004215540247969329\n","[Valid] loss:0.0024757687933743\n","--- 246 Epoch ---\n","[Train] loss:0.0030225819209590554\n","[Valid] loss:0.0023858859203755856\n","--- 247 Epoch ---\n","[Train] loss:0.0037001067539677024\n","[Valid] loss:0.003270714543759823\n","--- 248 Epoch ---\n","[Train] loss:0.002766737889032811\n","[Valid] loss:0.0035590503830462694\n","--- 249 Epoch ---\n","[Train] loss:0.004122631158679724\n","[Valid] loss:0.003891470143571496\n","--- 250 Epoch ---\n","[Train] loss:0.003409623634070158\n","[Valid] loss:0.0033266490790992975\n","--- 251 Epoch ---\n","[Train] loss:0.004276625346392393\n","[Valid] loss:0.004081492777913809\n","--- 252 Epoch ---\n","[Train] loss:0.008898661471903324\n","[Valid] loss:0.00235097692348063\n","--- 253 Epoch ---\n","[Train] loss:0.0036956185940653086\n","[Valid] loss:0.005084843374788761\n","--- 254 Epoch ---\n","[Train] loss:0.009078089613467455\n","[Valid] loss:0.0028005687054246664\n","--- 255 Epoch ---\n","[Train] loss:0.005781588843092322\n","[Valid] loss:0.0038423161022365093\n","--- 256 Epoch ---\n","[Train] loss:0.003860849072225392\n","[Valid] loss:0.0036278660409152508\n","--- 257 Epoch ---\n","[Train] loss:0.004013435682281852\n","[Valid] loss:0.004771876148879528\n","--- 258 Epoch ---\n","[Train] loss:0.0043117874301970005\n","[Valid] loss:0.004476993344724178\n","--- 259 Epoch ---\n","[Train] loss:0.002396613359451294\n","[Valid] loss:0.0044103446416556835\n","--- 260 Epoch ---\n","[Train] loss:0.0033805961720645428\n","[Valid] loss:0.004801235627382994\n","--- 261 Epoch ---\n","[Train] loss:0.0029099175008013844\n","[Valid] loss:0.0026340989861637354\n","--- 262 Epoch ---\n","[Train] loss:0.007497781654819846\n","[Valid] loss:0.0025753825902938843\n","--- 263 Epoch ---\n","[Train] loss:0.013114497065544128\n","[Valid] loss:0.004508558195084333\n","--- 264 Epoch ---\n","[Train] loss:0.0030980794690549374\n","[Valid] loss:0.005089269485324621\n","--- 265 Epoch ---\n","[Train] loss:0.009337573312222958\n","[Valid] loss:0.003722614608705044\n","--- 266 Epoch ---\n","[Train] loss:0.004481650423258543\n","[Valid] loss:0.0034216518979519606\n","--- 267 Epoch ---\n","[Train] loss:0.002265791583340615\n","[Valid] loss:0.0037051208782941103\n","--- 268 Epoch ---\n","[Train] loss:0.004634557408280671\n","[Valid] loss:0.004743398167192936\n","--- 269 Epoch ---\n","[Train] loss:0.0034214374609291553\n","[Valid] loss:0.0030017963144928217\n","--- 270 Epoch ---\n","[Train] loss:0.006313013145700097\n","[Valid] loss:0.002608593087643385\n","--- 271 Epoch ---\n","[Train] loss:0.004813851788640022\n","[Valid] loss:0.0037073486018925905\n","--- 272 Epoch ---\n","[Train] loss:0.004478837596252561\n","[Valid] loss:0.004871196113526821\n","--- 273 Epoch ---\n","[Train] loss:0.007222331594675779\n","[Valid] loss:0.003770862938836217\n","--- 274 Epoch ---\n","[Train] loss:0.004062782158143818\n","[Valid] loss:0.0027647342067211866\n","--- 275 Epoch ---\n","[Train] loss:0.009488342213444412\n","[Valid] loss:0.0034562558867037296\n","--- 276 Epoch ---\n","[Train] loss:0.003456228063441813\n","[Valid] loss:0.0033012819476425648\n","--- 277 Epoch ---\n","[Train] loss:0.0035449498100206256\n","[Valid] loss:0.003453096142038703\n","--- 278 Epoch ---\n","[Train] loss:0.0033469932386651635\n","[Valid] loss:0.005221018102020025\n","--- 279 Epoch ---\n","[Train] loss:0.0035628991900011897\n","[Valid] loss:0.004312044940888882\n","--- 280 Epoch ---\n","[Train] loss:0.0033348232973366976\n","[Valid] loss:0.0021690120920538902\n","--- 281 Epoch ---\n","[Train] loss:0.004137848853133619\n","[Valid] loss:0.0038210272323340178\n","--- 282 Epoch ---\n","[Train] loss:0.003072650928515941\n","[Valid] loss:0.003893630811944604\n","--- 283 Epoch ---\n","[Train] loss:0.005311273504048586\n","[Valid] loss:0.003368892939761281\n","--- 284 Epoch ---\n","[Train] loss:0.002721995231695473\n","[Valid] loss:0.0029294034466147423\n","--- 285 Epoch ---\n","[Train] loss:0.002932472387328744\n","[Valid] loss:0.00417579198256135\n","--- 286 Epoch ---\n","[Train] loss:0.009554599644616246\n","[Valid] loss:0.0026774629950523376\n","--- 287 Epoch ---\n","[Train] loss:0.0025128882261924446\n","[Valid] loss:0.004638952203094959\n","--- 288 Epoch ---\n","[Train] loss:0.006037665298208594\n","[Valid] loss:0.004573894664645195\n","--- 289 Epoch ---\n","[Train] loss:0.008796133333817124\n","[Valid] loss:0.004690564703196287\n","--- 290 Epoch ---\n","[Train] loss:0.006019814172759652\n","[Valid] loss:0.005028354935348034\n","--- 291 Epoch ---\n","[Train] loss:0.003458018065430224\n","[Valid] loss:0.004473366774618626\n","--- 292 Epoch ---\n","[Train] loss:0.00775544042699039\n","[Valid] loss:0.005626575089991093\n","--- 293 Epoch ---\n","[Train] loss:0.003935101791284978\n","[Valid] loss:0.003486014436930418\n","--- 294 Epoch ---\n","[Train] loss:0.004048585658892989\n","[Valid] loss:0.005686057265847921\n","--- 295 Epoch ---\n","[Train] loss:0.004239570000208914\n","[Valid] loss:0.003194899996742606\n","--- 296 Epoch ---\n","[Train] loss:0.0022057180758565664\n","[Valid] loss:0.0031428788788616657\n","--- 297 Epoch ---\n","[Train] loss:0.0026844447711482644\n","[Valid] loss:0.004242174793034792\n","--- 298 Epoch ---\n","[Train] loss:0.002520015405025333\n","[Valid] loss:0.003647629637271166\n","--- 299 Epoch ---\n","[Train] loss:0.006007705815136433\n","[Valid] loss:0.0030256116297096014\n","--- 300 Epoch ---\n","[Train] loss:0.002691025089006871\n","[Valid] loss:0.003416739869862795\n","--- 301 Epoch ---\n","[Train] loss:0.008123970357701182\n","[Valid] loss:0.00709607545286417\n","--- 302 Epoch ---\n","[Train] loss:0.005049607949331403\n","[Valid] loss:0.0028074400033801794\n","--- 303 Epoch ---\n","[Train] loss:0.0035778559395112097\n","[Valid] loss:0.004334199242293835\n","--- 304 Epoch ---\n","[Train] loss:0.004399056313559413\n","[Valid] loss:0.0036492724902927876\n","--- 305 Epoch ---\n","[Train] loss:0.003610487561672926\n","[Valid] loss:0.0045572081580758095\n","--- 306 Epoch ---\n","[Train] loss:0.002452245564199984\n","[Valid] loss:0.00399321224540472\n","--- 307 Epoch ---\n","[Train] loss:0.0021162835182622075\n","[Valid] loss:0.004110879730433226\n","--- 308 Epoch ---\n","[Train] loss:0.003855667542666197\n","[Valid] loss:0.002469919389113784\n","--- 309 Epoch ---\n","[Train] loss:0.008992931456305087\n","[Valid] loss:0.0045756082981824875\n","--- 310 Epoch ---\n","[Train] loss:0.0035647029289975762\n","[Valid] loss:0.0037075758446007967\n","--- 311 Epoch ---\n","[Train] loss:0.003499674377962947\n","[Valid] loss:0.005111761391162872\n","--- 312 Epoch ---\n","[Train] loss:0.0025994961615651846\n","[Valid] loss:0.004805717151612043\n","--- 313 Epoch ---\n","[Train] loss:0.0038038691272959113\n","[Valid] loss:0.0041699823923408985\n","--- 314 Epoch ---\n","[Train] loss:0.007798302220180631\n","[Valid] loss:0.0019274175865575671\n","--- 315 Epoch ---\n","[Train] loss:0.014364249538630247\n","[Valid] loss:0.0049522025510668755\n","--- 316 Epoch ---\n","[Train] loss:0.0030299106147140265\n","[Valid] loss:0.0025053112767636776\n","--- 317 Epoch ---\n","[Train] loss:0.005328456172719598\n","[Valid] loss:0.004241629038006067\n","--- 318 Epoch ---\n","[Train] loss:0.004501292947679758\n","[Valid] loss:0.00606850441545248\n","--- 319 Epoch ---\n","[Train] loss:0.003724870621226728\n","[Valid] loss:0.00379837560467422\n","--- 320 Epoch ---\n","[Train] loss:0.0021933246171101928\n","[Valid] loss:0.004313058685511351\n","--- 321 Epoch ---\n","[Train] loss:0.006449207779951394\n","[Valid] loss:0.004300003871321678\n","--- 322 Epoch ---\n","[Train] loss:0.003431066987104714\n","[Valid] loss:0.00483365636318922\n","--- 323 Epoch ---\n","[Train] loss:0.004431384266354144\n","[Valid] loss:0.00205372110940516\n","--- 324 Epoch ---\n","[Train] loss:0.0029798297327943146\n","[Valid] loss:0.004172829445451498\n","--- 325 Epoch ---\n","[Train] loss:0.0019787023193202913\n","[Valid] loss:0.0020973780192434788\n","--- 326 Epoch ---\n","[Train] loss:0.00466336845420301\n","[Valid] loss:0.0031639812514185905\n","--- 327 Epoch ---\n","[Train] loss:0.003979983041062951\n","[Valid] loss:0.003825374413281679\n","--- 328 Epoch ---\n","[Train] loss:0.005192036856897175\n","[Valid] loss:0.0028356711845844984\n","--- 329 Epoch ---\n","[Train] loss:0.002657353412359953\n","[Valid] loss:0.002352628391236067\n","--- 330 Epoch ---\n","[Train] loss:0.002485732431523502\n","[Valid] loss:0.0034734676592051983\n","--- 331 Epoch ---\n","[Train] loss:0.0034434666158631444\n","[Valid] loss:0.004461353179067373\n","--- 332 Epoch ---\n","[Train] loss:0.004411646164953709\n","[Valid] loss:0.0034384827595204115\n","--- 333 Epoch ---\n","[Train] loss:0.004554285318590701\n","[Valid] loss:0.0035217860713601112\n","--- 334 Epoch ---\n","[Train] loss:0.0038544845301657915\n","[Valid] loss:0.003074443666264415\n","--- 335 Epoch ---\n","[Train] loss:0.0031537411268800497\n","[Valid] loss:0.0035837600007653236\n","--- 336 Epoch ---\n","[Train] loss:0.0028314187075011432\n","[Valid] loss:0.0023749584797769785\n","--- 337 Epoch ---\n","[Train] loss:0.003939780639484525\n","[Valid] loss:0.0034932196140289307\n","--- 338 Epoch ---\n","[Train] loss:0.004450286156497896\n","[Valid] loss:0.0021733923349529505\n","--- 339 Epoch ---\n","[Train] loss:0.00432285072747618\n","[Valid] loss:0.0041452753357589245\n","--- 340 Epoch ---\n","[Train] loss:0.0023297201842069626\n","[Valid] loss:0.0037771151401102543\n","--- 341 Epoch ---\n","[Train] loss:0.002100359764881432\n","[Valid] loss:0.0035473231691867113\n","--- 342 Epoch ---\n","[Train] loss:0.0024926982005126774\n","[Valid] loss:0.002092232694849372\n","--- 343 Epoch ---\n","[Train] loss:0.0036063711158931255\n","[Valid] loss:0.004541711416095495\n","--- 344 Epoch ---\n","[Train] loss:0.004690732224844396\n","[Valid] loss:0.003996192943304777\n","--- 345 Epoch ---\n","[Train] loss:0.0028602182865142822\n","[Valid] loss:0.003270564367994666\n","--- 346 Epoch ---\n","[Train] loss:0.004619551706127822\n","[Valid] loss:0.0036444643046706915\n","--- 347 Epoch ---\n","[Train] loss:0.0027175676077604294\n","[Valid] loss:0.003895331872627139\n","--- 348 Epoch ---\n","[Train] loss:0.005693970946595073\n","[Valid] loss:0.003333136672154069\n","--- 349 Epoch ---\n","[Train] loss:0.01001534704118967\n","[Valid] loss:0.003870409680530429\n","--- 350 Epoch ---\n","[Train] loss:0.005714009050279856\n","[Valid] loss:0.00373127032071352\n","--- 351 Epoch ---\n","[Train] loss:0.004336111596785486\n","[Valid] loss:0.0029846897814422846\n","--- 352 Epoch ---\n","[Train] loss:0.0021235213498584926\n","[Valid] loss:0.004060064908117056\n","--- 353 Epoch ---\n","[Train] loss:0.00447554187849164\n","[Valid] loss:0.00444456422701478\n","--- 354 Epoch ---\n","[Train] loss:0.0033594511914998293\n","[Valid] loss:0.0043331473134458065\n","--- 355 Epoch ---\n","[Train] loss:0.00332654983503744\n","[Valid] loss:0.0056061395443975925\n","--- 356 Epoch ---\n","[Train] loss:0.003292211447842419\n","[Valid] loss:0.0032441767398267984\n","--- 357 Epoch ---\n","[Train] loss:0.0073020197451114655\n","[Valid] loss:0.00392439728602767\n","--- 358 Epoch ---\n","[Train] loss:0.014511630171909928\n","[Valid] loss:0.005875197239220142\n","--- 359 Epoch ---\n","[Train] loss:0.0035396141465753317\n","[Valid] loss:0.00593922333791852\n","--- 360 Epoch ---\n","[Train] loss:0.00485124031547457\n","[Valid] loss:0.002920094644650817\n","--- 361 Epoch ---\n","[Train] loss:0.004132997884880751\n","[Valid] loss:0.004060880746692419\n","--- 362 Epoch ---\n","[Train] loss:0.0034819378633983433\n","[Valid] loss:0.005385109223425388\n","--- 363 Epoch ---\n","[Train] loss:0.006528071127831936\n","[Valid] loss:0.0056066918186843395\n","--- 364 Epoch ---\n","[Train] loss:0.003503493615426123\n","[Valid] loss:0.005265542771667242\n","--- 365 Epoch ---\n","[Train] loss:0.002888531074859202\n","[Valid] loss:0.006270423531532288\n","--- 366 Epoch ---\n","[Train] loss:0.004871787503361702\n","[Valid] loss:0.0037271850742399693\n","--- 367 Epoch ---\n","[Train] loss:0.006277589709497988\n","[Valid] loss:0.0038202060386538506\n","--- 368 Epoch ---\n","[Train] loss:0.006784797005821019\n","[Valid] loss:0.0034320168197155\n","--- 369 Epoch ---\n","[Train] loss:0.0064188443357124925\n","[Valid] loss:0.0025225281715393066\n","--- 370 Epoch ---\n","[Train] loss:0.005102959112264216\n","[Valid] loss:0.0037550320848822594\n","--- 371 Epoch ---\n","[Train] loss:0.0058038735296577215\n","[Valid] loss:0.003229093039408326\n","--- 372 Epoch ---\n","[Train] loss:0.003024319012183696\n","[Valid] loss:0.003110375953838229\n","--- 373 Epoch ---\n","[Train] loss:0.0033785433042794466\n","[Valid] loss:0.0025765185710042715\n","--- 374 Epoch ---\n","[Train] loss:0.0026992957573384047\n","[Valid] loss:0.0032017093617469072\n","--- 375 Epoch ---\n","[Train] loss:0.003265010076574981\n","[Valid] loss:0.003342933487147093\n","--- 376 Epoch ---\n","[Train] loss:0.002871524542570114\n","[Valid] loss:0.003198288381099701\n","--- 377 Epoch ---\n","[Train] loss:0.003811060800217092\n","[Valid] loss:0.0021960518788546324\n","--- 378 Epoch ---\n","[Train] loss:0.0031621234957128763\n","[Valid] loss:0.0031731007620692253\n","--- 379 Epoch ---\n","[Train] loss:0.004210793413221836\n","[Valid] loss:0.004082871600985527\n","--- 380 Epoch ---\n","[Train] loss:0.003523224499076605\n","[Valid] loss:0.0026819342747330666\n","--- 381 Epoch ---\n","[Train] loss:0.003718792402651161\n","[Valid] loss:0.002798933768644929\n","--- 382 Epoch ---\n","[Train] loss:0.001979861583095044\n","[Valid] loss:0.002836178056895733\n","--- 383 Epoch ---\n","[Train] loss:0.002157983893994242\n","[Valid] loss:0.0027332166209816933\n","--- 384 Epoch ---\n","[Train] loss:0.0022637633373960853\n","[Valid] loss:0.0048446450382471085\n","--- 385 Epoch ---\n","[Train] loss:0.0029405399691313505\n","[Valid] loss:0.0030949688516557217\n","--- 386 Epoch ---\n","[Train] loss:0.0014405188267119229\n","[Valid] loss:0.003751257201656699\n","--- 387 Epoch ---\n","[Train] loss:0.0015256933984346688\n","[Valid] loss:0.0018668503034859896\n","--- 388 Epoch ---\n","[Train] loss:0.0019893149146810174\n","[Valid] loss:0.0029081935063004494\n","--- 389 Epoch ---\n","[Train] loss:0.0020455160411074758\n","[Valid] loss:0.0027704983949661255\n","--- 390 Epoch ---\n","[Train] loss:0.0029997382080182433\n","[Valid] loss:0.0030826476868242025\n","--- 391 Epoch ---\n","[Train] loss:0.0030703849624842405\n","[Valid] loss:0.0039227488450706005\n","--- 392 Epoch ---\n","[Train] loss:0.0033400742104277015\n","[Valid] loss:0.0015016940888017416\n","--- 393 Epoch ---\n","[Train] loss:0.0026383106596767902\n","[Valid] loss:0.004993164446204901\n","--- 394 Epoch ---\n","[Train] loss:0.003236703691072762\n","[Valid] loss:0.0036015668883919716\n","--- 395 Epoch ---\n","[Train] loss:0.0020626565092243254\n","[Valid] loss:0.003212416311725974\n","--- 396 Epoch ---\n","[Train] loss:0.0023393596638925374\n","[Valid] loss:0.0031538226176053286\n","--- 397 Epoch ---\n","[Train] loss:0.0028718673856928945\n","[Valid] loss:0.0030825664289295673\n","--- 398 Epoch ---\n","[Train] loss:0.0023965301224961877\n","[Valid] loss:0.004212165251374245\n","--- 399 Epoch ---\n","[Train] loss:0.0038536397041752934\n","[Valid] loss:0.0031920636538416147\n","--- 400 Epoch ---\n","[Train] loss:0.0036406364524737\n","[Valid] loss:0.004212457221001387\n","--- 401 Epoch ---\n","[Train] loss:0.002169810817576945\n","[Valid] loss:0.003433010308071971\n","--- 402 Epoch ---\n","[Train] loss:0.0031520448392257094\n","[Valid] loss:0.002217172645032406\n","--- 403 Epoch ---\n","[Train] loss:0.0033083531889133155\n","[Valid] loss:0.004658577032387257\n","--- 404 Epoch ---\n","[Train] loss:0.0019685268052853644\n","[Valid] loss:0.005229146219789982\n","--- 405 Epoch ---\n","[Train] loss:0.003557102638296783\n","[Valid] loss:0.0037427374627441168\n","--- 406 Epoch ---\n","[Train] loss:0.0035188975743949413\n","[Valid] loss:0.003556974232196808\n","--- 407 Epoch ---\n","[Train] loss:0.0078057951759546995\n","[Valid] loss:0.003879052819684148\n","--- 408 Epoch ---\n","[Train] loss:0.0034429862862452865\n","[Valid] loss:0.00202449900098145\n","--- 409 Epoch ---\n","[Train] loss:0.003363771247677505\n","[Valid] loss:0.005426372401416302\n","--- 410 Epoch ---\n","[Train] loss:0.005707614705897868\n","[Valid] loss:0.004556022118777037\n","--- 411 Epoch ---\n","[Train] loss:0.005463623150717467\n","[Valid] loss:0.003092788625508547\n","--- 412 Epoch ---\n","[Train] loss:0.0021737232455052435\n","[Valid] loss:0.002682680729776621\n","--- 413 Epoch ---\n","[Train] loss:0.006260817754082382\n","[Valid] loss:0.004343381617218256\n","--- 414 Epoch ---\n","[Train] loss:0.00240580877289176\n","[Valid] loss:0.0032163350842893124\n","--- 415 Epoch ---\n","[Train] loss:0.0027096265694126487\n","[Valid] loss:0.002307281829416752\n","--- 416 Epoch ---\n","[Train] loss:0.0036084314342588186\n","[Valid] loss:0.003354880027472973\n","--- 417 Epoch ---\n","[Train] loss:0.002734641486313194\n","[Valid] loss:0.001605503843165934\n","--- 418 Epoch ---\n","[Train] loss:0.0024334269110113382\n","[Valid] loss:0.004195588640868664\n","--- 419 Epoch ---\n","[Train] loss:0.002010397962294519\n","[Valid] loss:0.002955307951197028\n","--- 420 Epoch ---\n","[Train] loss:0.002310346288140863\n","[Valid] loss:0.0031291444320231676\n","--- 421 Epoch ---\n","[Train] loss:0.004609178169630468\n","[Valid] loss:0.003960389643907547\n","--- 422 Epoch ---\n","[Train] loss:0.0028376313857734203\n","[Valid] loss:0.002459157956764102\n","--- 423 Epoch ---\n","[Train] loss:0.003563922829926014\n","[Valid] loss:0.002215457847341895\n","--- 424 Epoch ---\n","[Train] loss:0.004262565169483423\n","[Valid] loss:0.004701230209320784\n","--- 425 Epoch ---\n","[Train] loss:0.002141075092367828\n","[Valid] loss:0.003826146014034748\n","--- 426 Epoch ---\n","[Train] loss:0.002478337613865733\n","[Valid] loss:0.003232516348361969\n","--- 427 Epoch ---\n","[Train] loss:0.004887895425781608\n","[Valid] loss:0.0035421857610344887\n","--- 428 Epoch ---\n","[Train] loss:0.001632007013540715\n","[Valid] loss:0.0039756507612764835\n","--- 429 Epoch ---\n","[Train] loss:0.0019537367625162005\n","[Valid] loss:0.003967846743762493\n","--- 430 Epoch ---\n","[Train] loss:0.0018561925389803946\n","[Valid] loss:0.0017555321101099253\n","--- 431 Epoch ---\n","[Train] loss:0.010273644234985113\n","[Valid] loss:0.0035448672715574503\n","--- 432 Epoch ---\n","[Train] loss:0.0014114482328295708\n","[Valid] loss:0.00316077983006835\n","--- 433 Epoch ---\n","[Train] loss:0.003046095196623355\n","[Valid] loss:0.003668396035209298\n","--- 434 Epoch ---\n","[Train] loss:0.00235300837084651\n","[Valid] loss:0.003744626883417368\n","--- 435 Epoch ---\n","[Train] loss:0.001668252982199192\n","[Valid] loss:0.004340513609349728\n","--- 436 Epoch ---\n","[Train] loss:0.002100538869854063\n","[Valid] loss:0.0024711666628718376\n","--- 437 Epoch ---\n","[Train] loss:0.003148020477965474\n","[Valid] loss:0.0026746795047074556\n","--- 438 Epoch ---\n","[Train] loss:0.0030052054207772017\n","[Valid] loss:0.0038854582235217094\n","--- 439 Epoch ---\n","[Train] loss:0.002639391808770597\n","[Valid] loss:0.0032527674920856953\n","--- 440 Epoch ---\n","[Train] loss:0.0019205869757570326\n","[Valid] loss:0.0021271700970828533\n","--- 441 Epoch ---\n","[Train] loss:0.006313034100458026\n","[Valid] loss:0.005551540292799473\n","--- 442 Epoch ---\n","[Train] loss:0.003409947908949107\n","[Valid] loss:0.0020967265591025352\n","--- 443 Epoch ---\n","[Train] loss:0.0027619502507150173\n","[Valid] loss:0.0031839075963944197\n","--- 444 Epoch ---\n","[Train] loss:0.006579654756933451\n","[Valid] loss:0.004403571132570505\n","--- 445 Epoch ---\n","[Train] loss:0.0018634544685482979\n","[Valid] loss:0.003659477923065424\n","--- 446 Epoch ---\n","[Train] loss:0.0027787413564510643\n","[Valid] loss:0.0021670456044375896\n","--- 447 Epoch ---\n","[Train] loss:0.0023823733208701015\n","[Valid] loss:0.002845864510163665\n","--- 448 Epoch ---\n","[Train] loss:0.002377172582782805\n","[Valid] loss:0.0024317423813045025\n","--- 449 Epoch ---\n","[Train] loss:0.0023725933860987425\n","[Valid] loss:0.003924310673028231\n","--- 450 Epoch ---\n","[Train] loss:0.002752298431005329\n","[Valid] loss:0.0043915132991969585\n","--- 451 Epoch ---\n","[Train] loss:0.0025376168196089566\n","[Valid] loss:0.002678587334230542\n","--- 452 Epoch ---\n","[Train] loss:0.003472103155218065\n","[Valid] loss:0.0038886540569365025\n","--- 453 Epoch ---\n","[Train] loss:0.0017806035466492176\n","[Valid] loss:0.003782642772421241\n","--- 454 Epoch ---\n","[Train] loss:0.005045069032348692\n","[Valid] loss:0.0025766112376004457\n","--- 455 Epoch ---\n","[Train] loss:0.0024614568101242185\n","[Valid] loss:0.003342500189319253\n","--- 456 Epoch ---\n","[Train] loss:0.005190209834836423\n","[Valid] loss:0.0038339728489518166\n","--- 457 Epoch ---\n","[Train] loss:0.0025946077657863498\n","[Valid] loss:0.004329979885369539\n","--- 458 Epoch ---\n","[Train] loss:0.004453921923413873\n","[Valid] loss:0.0027135645505040884\n","--- 459 Epoch ---\n","[Train] loss:0.004059245926328003\n","[Valid] loss:0.0020105584990233183\n","--- 460 Epoch ---\n","[Train] loss:0.003123891889117658\n","[Valid] loss:0.0030956093687564135\n","--- 461 Epoch ---\n","[Train] loss:0.0032380936900153756\n","[Valid] loss:0.0032766913063824177\n","--- 462 Epoch ---\n","[Train] loss:0.0022408983786590397\n","[Valid] loss:0.006685980129987001\n","--- 463 Epoch ---\n","[Train] loss:0.0020038814400322735\n","[Valid] loss:0.0035235551185905933\n","--- 464 Epoch ---\n","[Train] loss:0.001722749788314104\n","[Valid] loss:0.0036245526280254126\n","--- 465 Epoch ---\n","[Train] loss:0.003005404374562204\n","[Valid] loss:0.0042661637999117374\n","--- 466 Epoch ---\n","[Train] loss:0.002978718373924494\n","[Valid] loss:0.002741775009781122\n","--- 467 Epoch ---\n","[Train] loss:0.0025975388707593083\n","[Valid] loss:0.0036543423775583506\n","--- 468 Epoch ---\n","[Train] loss:0.0018246689578518271\n","[Valid] loss:0.0050112782046198845\n","--- 469 Epoch ---\n","[Train] loss:0.006212550797499716\n","[Valid] loss:0.003298145718872547\n","--- 470 Epoch ---\n","[Train] loss:0.007647031103260815\n","[Valid] loss:0.004053667653352022\n","--- 471 Epoch ---\n","[Train] loss:0.0020792323630303144\n","[Valid] loss:0.00452799629420042\n","--- 472 Epoch ---\n","[Train] loss:0.003411298501305282\n","[Valid] loss:0.002710528438910842\n","--- 473 Epoch ---\n","[Train] loss:0.006445289356634021\n","[Valid] loss:0.004290311131626368\n","--- 474 Epoch ---\n","[Train] loss:0.002478480455465615\n","[Valid] loss:0.004723923280835152\n","--- 475 Epoch ---\n","[Train] loss:0.004669043701142073\n","[Valid] loss:0.003989184275269508\n","--- 476 Epoch ---\n","[Train] loss:0.003464906709268689\n","[Valid] loss:0.0030445365700870752\n","--- 477 Epoch ---\n","[Train] loss:0.002221311326138675\n","[Valid] loss:0.00284741073846817\n","--- 478 Epoch ---\n","[Train] loss:0.002805117517709732\n","[Valid] loss:0.002536321757361293\n","--- 479 Epoch ---\n","[Train] loss:0.0021072732051834464\n","[Valid] loss:0.0018854178488254547\n","--- 480 Epoch ---\n","[Train] loss:0.0030382006661966443\n","[Valid] loss:0.005070177838206291\n","--- 481 Epoch ---\n","[Train] loss:0.0017741411575116217\n","[Valid] loss:0.0033959189895540476\n","--- 482 Epoch ---\n","[Train] loss:0.0039842750993557274\n","[Valid] loss:0.003356437897309661\n","--- 483 Epoch ---\n","[Train] loss:0.004433668567799032\n","[Valid] loss:0.004569200333207846\n","--- 484 Epoch ---\n","[Train] loss:0.003364574280567467\n","[Valid] loss:0.0040199728682637215\n","--- 485 Epoch ---\n","[Train] loss:0.0038989681052044034\n","[Valid] loss:0.003967598080635071\n","--- 486 Epoch ---\n","[Train] loss:0.005672485334798694\n","[Valid] loss:0.0022444084752351046\n","--- 487 Epoch ---\n","[Train] loss:0.003849940258078277\n","[Valid] loss:0.0032410037238150835\n","--- 488 Epoch ---\n","[Train] loss:0.002236420870758593\n","[Valid] loss:0.001974707003682852\n","--- 489 Epoch ---\n","[Train] loss:0.0020639168797060847\n","[Valid] loss:0.002875610487535596\n","--- 490 Epoch ---\n","[Train] loss:0.002469303086400032\n","[Valid] loss:0.0024081792216748\n","--- 491 Epoch ---\n","[Train] loss:0.00456621526973322\n","[Valid] loss:0.0032057417556643486\n","--- 492 Epoch ---\n","[Train] loss:0.0028729342157021165\n","[Valid] loss:0.001501670340076089\n","--- 493 Epoch ---\n","[Train] loss:0.0020793398143723607\n","[Valid] loss:0.0023503010161221027\n","--- 494 Epoch ---\n","[Train] loss:0.0028268799651414156\n","[Valid] loss:0.0028981217183172703\n","--- 495 Epoch ---\n","[Train] loss:0.00306906271725893\n","[Valid] loss:0.003516072640195489\n","--- 496 Epoch ---\n","[Train] loss:0.004522549454122782\n","[Valid] loss:0.00246377382427454\n","--- 497 Epoch ---\n","[Train] loss:0.002731095999479294\n","[Valid] loss:0.0023650345392525196\n","--- 498 Epoch ---\n","[Train] loss:0.0022907740203663707\n","[Valid] loss:0.0031241129618138075\n","--- 499 Epoch ---\n","[Train] loss:0.002590083808172494\n","[Valid] loss:0.0028166985139250755\n","--- 500 Epoch ---\n","[Train] loss:0.003690959536470473\n","[Valid] loss:0.004951215349137783\n","--- finish ---\n","The best epoch of valid loss : Epoch:{} 492\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 0 Axes>"]},"metadata":{}}],"source":["for epoch in range(EPOCH):\n","    print('--- {} Epoch ---'.format(epoch+1))\n","    train_valid('train',net,train_loader,loss_function,optim,train_history)\n","    train_valid('valid',net,valid_loader,loss_function,optim,valid_history)\n","    \n","    if min(valid_history['loss']) == valid_history['loss'][-1]:\n","        torch.save(net.state_dict(), OUTPUT_DIR + 'train/' +'best_model.pth')\n","\n","    if (epoch+1) % 50 == 0:\n","        plot_graph(train_history['loss'], valid_history['loss'])\n","\n","print('--- finish ---')\n","epoch = valid_history['loss'].index(min(valid_history['loss']))+1\n","print('The best epoch of valid loss : Epoch:{}', epoch)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"}},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}