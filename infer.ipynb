{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: \n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANSWER_SIZE = 128\n",
    "WINDOW_SIZE = int(ANSWER_SIZE / 2)\n",
    "MINI_BATCH = 10\n",
    "EPOCH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 images loaded.(train:valid=20:10)\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./dataset/images/*')\n",
    "train_files = files[0:20]\n",
    "valid_files = files[20:30]\n",
    "print('{} images loaded.(train:valid={}:{})'.format(len(files),len(train_files),len(valid_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSampleNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UpSampleNet, self).__init__()\n",
    "        self.input = self.output = 0\n",
    "        self.layer1 = self.layer2 = self.layer3 = self.layer4 = self.layer5 = 0\n",
    "        \n",
    "        self.step1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3,out_channels=32,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.step2 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            torch.nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.step3 = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            torch.nn.Conv2d(in_channels=64,out_channels=128,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(128), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=128,out_channels=128,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(128), torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.deconv1 = torch.nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
    "\n",
    "        self.step4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=128,out_channels=64,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=64,out_channels=64,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(64), torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.deconv2 = torch.nn.ConvTranspose2d(in_channels=64,out_channels=32,kernel_size=2,stride=2)\n",
    "\n",
    "        self.step5 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=64,out_channels=32,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=32,out_channels=32,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(32), torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.deconv3 = torch.nn.ConvTranspose2d(in_channels=32,out_channels=16,kernel_size=2,stride=2)\n",
    "\n",
    "        self.step6 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16,out_channels=8,kernel_size=3,padding=1),\n",
    "            torch.nn.BatchNorm2d(8), torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(in_channels=8,out_channels=3,kernel_size=3,padding=1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, small):\n",
    "        self.input = small\n",
    "        # Encode\n",
    "        self.layer1 = self.step1(self.input)\n",
    "        self.layer2 = self.step2(self.layer1)\n",
    "        self.layer3 = self.step3(self.layer2)\n",
    "        # Decode\n",
    "        self.layer3 = self.deconv1(self.layer3)\n",
    "        self.layer4 = self.step4(torch.cat((self.layer3,self.layer2),dim=1))\n",
    "        self.layer4 = self.deconv2(self.layer4)\n",
    "        self.layer5 = self.step5(torch.cat((self.layer4,self.layer1),dim=1))\n",
    "        self.layer5 = self.deconv3(self.layer5)\n",
    "        self.output = self.step6(self.layer5)\n",
    "        return self.output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, small, padding=WINDOW_SIZE, stride=WINDOW_SIZE):\n",
    "    model.train(False)\n",
    "    h,w = len(small[0]), len(small[0,0])\n",
    "    small = torchvision.transforms.functional.pad(small, padding)\n",
    "    large = torch.zeros(len(small),len(small[0])*2,len(small[0,0])*2)\n",
    "\n",
    "    top = 0\n",
    "    while top + WINDOW_SIZE <= len(small[0]):\n",
    "        left = 0\n",
    "        while left + WINDOW_SIZE <= len(small[0,0]):\n",
    "            input = torchvision.transforms.functional.crop(small,top,left,WINDOW_SIZE,WINDOW_SIZE)\n",
    "            large[:,top*2:top*2+ANSWER_SIZE,left*2:left*2+ANSWER_SIZE] = model(torch.unsqueeze(input,dim=0))\n",
    "            left += stride\n",
    "        top += stride\n",
    "    \n",
    "    large = torchvision.transforms.functional.center_crop(large,[h*2,w*2])\n",
    "    return torch.squeeze(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = UpSampleNet()\n",
    "net.load_state_dict(torch.load('./outputs/best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "for filename in valid_files:\n",
    "    image = Image.open(filename)\n",
    "    image = torchvision.transforms.functional.to_tensor(image)\n",
    "    \n",
    "    large_image = infer(net, image)\n",
    "    #torchvision.utils.save_image(large_image, './outputs/infer/'+filename.split('/')[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
